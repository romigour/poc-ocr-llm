# Lancer son LLM en local

- Installer Ollama
- Lancer le serveur Ollama `ollama serve` (Possiblement le fermer s'il est déjà lancé)
- Lancer le model souhaité `ollama run llama3.2-vision`

# Version MainLlamaVisionMistral

## LLM
- `llama3.2-vision` pour l'OCR
- `mistral` pour structurer les données

# Version MainTesseractNuExtract

## Tool
- Tess4j : https://sourceforge.net/projects/tess4j/ 

## LLM
- `nuextract` pour structurer les données

# LangChain4j

## Docs
- https://docs.langchain4j.dev/integrations/language-models/
- https://github.com/langchain4j/langchain4j-examples
- https://github.com/langchain4j/langchain4j-spring
- https://www.youtube.com/watch?v=0FEUmI2Ou10 (vidéo devoxx)